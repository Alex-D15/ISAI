{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import surprise\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from scipy.stats import percentileofscore\n",
    "import math\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from fastai.collab import * \n",
    "from fastai.tabular import *\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing as pre\n",
    "from surprise import SVD\n",
    "from surprise import KNNBasic\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2765830;\"(Aalborg) Andersen</th>\n",
       "      <th>Lucas \";6.74000000;1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3948313;\"(Aalborg) Andersen</td>\n",
       "      <td>Lucas \";2.73000000;1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10051046;\"(Aalborg) Andersen</td>\n",
       "      <td>Lucas \";4.69000000;1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>884724;\"(Aalborg) Kusk K \";5.12000000;1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3891694;\"(Aalborg) Kusk K \";3.24000000;1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>824001;\"(Aalborg) Prica</td>\n",
       "      <td>Tim \";2.45000000;1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                2765830;\"(Aalborg) Andersen   Lucas \";6.74000000;1\n",
       "0               3948313;\"(Aalborg) Andersen   Lucas \";2.73000000;1\n",
       "1              10051046;\"(Aalborg) Andersen   Lucas \";4.69000000;1\n",
       "2   884724;\"(Aalborg) Kusk K \";5.12000000;1                    NaN\n",
       "3  3891694;\"(Aalborg) Kusk K \";3.24000000;1                    NaN\n",
       "4                   824001;\"(Aalborg) Prica     Tim \";2.45000000;1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.ceci\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 13395033 ratings\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'IDUtente'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'IDUtente'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-287b667b5821>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'We have'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ratings'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The number of unique users we have is:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"IDUtente\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The number of unique teams we have is:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Team\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The median user rated %d teams.'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"IDUtente\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'IDUtente'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r'D:\\Projects\\ISAI\\Data\\SubEventsCF\\results.csv', skiprows=0)\n",
    "df.drop_duplicates(inplace=True)\n",
    "print('We have',df.shape[0], 'ratings')\n",
    "print('The number of unique users we have is:', len(df[\"IDUtente\"].unique()))\n",
    "print('The number of unique teams we have is:', len(df[\"Team\"].unique()))\n",
    "print('The median user rated %d teams.'%df[\"IDUtente\"].value_counts().median())\n",
    "print('The max rating is: %d'%df[\"Importo\"].max(),'the min rating is: %d'%df[\"Importo\"].min())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed in percentile calculation:  32.278406381607056 s\n",
      "Average time per user:  0.00016813508967963712 s\n"
     ]
    }
   ],
   "source": [
    "dftot=pd.read_csv(r'D:\\Projects\\ISAI\\Data\\SubEventsCF\\itemCFdata.csv', skiprows=0)\n",
    "dfnum=pd.read_csv(r'D:\\Projects\\ISAI\\Data\\SubEventsCF\\numerogiocateCF.csv', skiprows=0)\n",
    "\n",
    "##Check for teams that have been bet on less times than arbitrary value\n",
    "#min_inst = 1\n",
    "#team_count = dftot.value_counts('Team') < min_inst\n",
    "##Get list of teams to drop\n",
    "#teams_to_drop = team_count.where(team_count==True).dropna().index\n",
    "##Drop from dataframe\n",
    "#dftot = dftot[~dftot['Team'].isin(teams_to_drop)]\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "#Calculate, for each user, the percentile rank for each amount relative to the list of all the amounts wagered\n",
    "for user in set(dftot['IDUtente']):\n",
    "    importi = dftot.loc[dftot['IDUtente'] == user, 'Importo']\n",
    "    numero = dfnum.loc[dfnum['IDUtente'] == user, 'NumeroGiocate']\n",
    "\n",
    "    dftot.loc[dftot['IDUtente'] == user, 'Importo'] = [(alpha*(percentileofscore(importi, a, 'rank')/20) + beta*(percentileofscore(numero, b, 'rank')/20)) for a,b in zip(importi,numero)]\n",
    "    \n",
    "\n",
    "total_time = time()-start_time\n",
    "print('Time elapsed in percentile calculation: ', total_time, 's')\n",
    "print('Average time per user: ', total_time/len(set(df['IDUtente'])), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning:\n",
      "We have 185273 ratings\n",
      "The number of unique users we have is: 6706\n",
      "The number of unique teams we have is: 6085\n",
      "The median user rated 12 teams.\n",
      "The max rating is: 5 the min rating is: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDUtente</th>\n",
       "      <th>Team</th>\n",
       "      <th>Importo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1773</td>\n",
       "      <td>(Mercedes) L.Hamilton</td>\n",
       "      <td>1.427833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21249</td>\n",
       "      <td>(Mercedes) V.Bottas</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536</td>\n",
       "      <td>(Racing Point) S.Perez</td>\n",
       "      <td>3.558052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15746</td>\n",
       "      <td>07 Vestur</td>\n",
       "      <td>1.225394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20820</td>\n",
       "      <td>07 Vestur</td>\n",
       "      <td>2.217742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDUtente                    Team   Importo\n",
       "0      1773   (Mercedes) L.Hamilton  1.427833\n",
       "1     21249     (Mercedes) V.Bottas  5.000000\n",
       "2       536  (Racing Point) S.Perez  3.558052\n",
       "3     15746               07 Vestur  1.225394\n",
       "4     20820               07 Vestur  2.217742"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('After pruning:')\n",
    "print('We have',dftot.shape[0], 'ratings')\n",
    "print('The number of unique users we have is:', len(dftot[\"IDUtente\"].unique()))\n",
    "print('The number of unique teams we have is:', len(dftot[\"Team\"].unique()))\n",
    "print('The median user rated %d teams.'%dftot[\"IDUtente\"].value_counts().median())\n",
    "print('The max rating is: %d'%dftot[\"Importo\"].max(),'the min rating is: %d'%dftot[\"Importo\"].min())\n",
    "dftot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Team</th>\n",
       "      <th>(Mercedes) L.Hamilton</th>\n",
       "      <th>(Mercedes) V.Bottas</th>\n",
       "      <th>(Racing Point) S.Perez</th>\n",
       "      <th>07 Vestur</th>\n",
       "      <th>1 Dezembro</th>\n",
       "      <th>1. FFC Frankfurt Women</th>\n",
       "      <th>1860 Munich</th>\n",
       "      <th>1877 Alemdag</th>\n",
       "      <th>1922 Konyaspor</th>\n",
       "      <th>4 de Julho</th>\n",
       "      <th>...</th>\n",
       "      <th>Zlatibor</th>\n",
       "      <th>Znamya Noginsk</th>\n",
       "      <th>Zob Ahan</th>\n",
       "      <th>Zoe Hammond</th>\n",
       "      <th>Zonguldak Komurspor</th>\n",
       "      <th>Zoo Kericho FC</th>\n",
       "      <th>Zrinjski Mostar</th>\n",
       "      <th>Zvezda Ryazan</th>\n",
       "      <th>Zvezda St. Petersburg</th>\n",
       "      <th>Zweigen Kanazawa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDUtente</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>388.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21376.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21377.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21378.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21380.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21385.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6706 rows Ã— 6085 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Team      (Mercedes) L.Hamilton  (Mercedes) V.Bottas  (Racing Point) S.Perez  \\\n",
       "IDUtente                                                                       \n",
       "388.0                       NaN                  NaN                     NaN   \n",
       "391.0                       NaN                  NaN                     NaN   \n",
       "392.0                       NaN                  NaN                     NaN   \n",
       "393.0                       NaN                  NaN                     NaN   \n",
       "394.0                       NaN                  NaN                     NaN   \n",
       "...                         ...                  ...                     ...   \n",
       "21376.0                     NaN                  NaN                     NaN   \n",
       "21377.0                     NaN                  NaN                     NaN   \n",
       "21378.0                     NaN                  NaN                     NaN   \n",
       "21380.0                     NaN                  NaN                     NaN   \n",
       "21385.0                     NaN                  NaN                     NaN   \n",
       "\n",
       "Team      07 Vestur  1 Dezembro  1. FFC Frankfurt Women  1860 Munich  \\\n",
       "IDUtente                                                               \n",
       "388.0           NaN         NaN                     NaN          NaN   \n",
       "391.0           NaN         NaN                     NaN          NaN   \n",
       "392.0           NaN         NaN                     NaN          NaN   \n",
       "393.0           NaN         NaN                     NaN          NaN   \n",
       "394.0           NaN         NaN                     NaN          NaN   \n",
       "...             ...         ...                     ...          ...   \n",
       "21376.0         NaN         NaN                     NaN          NaN   \n",
       "21377.0         NaN         NaN                     NaN          NaN   \n",
       "21378.0         NaN         NaN                     NaN          NaN   \n",
       "21380.0         NaN         NaN                     NaN          NaN   \n",
       "21385.0         NaN         NaN                     NaN          NaN   \n",
       "\n",
       "Team      1877 Alemdag  1922 Konyaspor  4 de Julho  ...  Zlatibor  \\\n",
       "IDUtente                                            ...             \n",
       "388.0              NaN             NaN         NaN  ...       NaN   \n",
       "391.0              NaN             NaN         NaN  ...       NaN   \n",
       "392.0              NaN             NaN         NaN  ...       NaN   \n",
       "393.0              NaN             NaN         NaN  ...       NaN   \n",
       "394.0              NaN             NaN         NaN  ...       NaN   \n",
       "...                ...             ...         ...  ...       ...   \n",
       "21376.0            NaN             NaN         NaN  ...       NaN   \n",
       "21377.0            NaN             NaN         NaN  ...       NaN   \n",
       "21378.0            NaN             NaN         NaN  ...       NaN   \n",
       "21380.0            NaN             NaN         NaN  ...       NaN   \n",
       "21385.0            NaN             NaN         NaN  ...       NaN   \n",
       "\n",
       "Team      Znamya Noginsk  Zob Ahan  Zoe Hammond  Zonguldak Komurspor  \\\n",
       "IDUtente                                                               \n",
       "388.0                NaN       NaN          NaN                  NaN   \n",
       "391.0                NaN       NaN          NaN                  NaN   \n",
       "392.0                NaN       NaN          NaN                  NaN   \n",
       "393.0                NaN       NaN          NaN                  NaN   \n",
       "394.0                NaN       NaN          NaN                  NaN   \n",
       "...                  ...       ...          ...                  ...   \n",
       "21376.0              NaN       NaN          NaN                  NaN   \n",
       "21377.0              NaN       NaN          NaN                  NaN   \n",
       "21378.0              NaN       NaN          NaN                  NaN   \n",
       "21380.0              NaN       NaN          NaN                  NaN   \n",
       "21385.0              NaN       NaN          NaN                  NaN   \n",
       "\n",
       "Team      Zoo Kericho FC  Zrinjski Mostar  Zvezda Ryazan  \\\n",
       "IDUtente                                                   \n",
       "388.0                NaN              NaN            NaN   \n",
       "391.0                NaN              NaN            NaN   \n",
       "392.0                NaN              NaN            NaN   \n",
       "393.0                NaN              NaN            NaN   \n",
       "394.0                NaN              NaN            NaN   \n",
       "...                  ...              ...            ...   \n",
       "21376.0              NaN              NaN            NaN   \n",
       "21377.0              NaN              NaN            NaN   \n",
       "21378.0              NaN              NaN            NaN   \n",
       "21380.0              NaN              NaN            NaN   \n",
       "21385.0              NaN              NaN            NaN   \n",
       "\n",
       "Team      Zvezda St. Petersburg  Zweigen Kanazawa  \n",
       "IDUtente                                           \n",
       "388.0                       NaN               NaN  \n",
       "391.0                       NaN               NaN  \n",
       "392.0                       NaN               NaN  \n",
       "393.0                       NaN               NaN  \n",
       "394.0                       NaN               NaN  \n",
       "...                         ...               ...  \n",
       "21376.0                     NaN               NaN  \n",
       "21377.0                     NaN               NaN  \n",
       "21378.0                     NaN               NaN  \n",
       "21380.0                     NaN               NaN  \n",
       "21385.0                     NaN               NaN  \n",
       "\n",
       "[6706 rows x 6085 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = df.pivot_table(index='IDUtente', columns='Team', values='Importo')\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dftot\n",
    "\n",
    "#swapping columns\n",
    "raw=df[['IDUtente','Team','Importo']] \n",
    "raw.columns = ['n_users','n_items','rating']\n",
    "\n",
    "rawTrain,rawholdout = train_test_split(raw, test_size=0.25)\n",
    "# when importing from a DF, you only need to specify the scale of the ratings.\n",
    "reader = surprise.Reader(rating_scale=(0,5))\n",
    "#into surprise:\n",
    "data = surprise.Dataset.load_from_df(rawTrain,reader)\n",
    "holdout = surprise.Dataset.load_from_df(rawholdout,reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Pseudo Code, our Algorithm is as follows:\n",
    "We split the dataset into 10 folds, where we train on 9 of the folds and test on the remaining one, which randomly alternates..\n",
    "We run several recommender systems on the dataset, and optimize the recommender systems on the 75% system.\n",
    "intialize a weighted variable alpha to be 1/q, where q is the number of recommender systems we use.\n",
    "let the rated matrix equal alpha * sum(predicted Ratings Matrices) and compare that with the real rating.\n",
    "Using Gradient Descent, optimize the alpha term over parameter space to be able to optimize to give the most weight to the model which can represent the best prediction.\n",
    "### First, lets pick some algorithms to include into our ensemble. We'll choose four.\n",
    "1. Collaborative Filtering\n",
    "2. Matrix Factorization\n",
    "3. Collaborative filtering with co-clustering\n",
    "4. Collaborative Filtering based on the popular Slope One Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into folds. \n",
    "kSplit = surprise.model_selection.split.KFold(n_splits=10, shuffle=True) \n",
    "#initialize error vectors\n",
    "rmseKNN = []\n",
    "rmseSVD = []\n",
    "rmseCo = []\n",
    "rmseSlope = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering classic KNN\n",
    "Number one on our list: Collaborative filtering is a recommender system that recommends based off of similiarity between items. The big idea is that items that are similiar should be similiarly liked by the same user. For example, if you liked Alien, and you really liked Predator, there's a good chance you'll enjoy Alien Versus Predator. We're just doing the same thing with books here. If you'd like to read more, read up here: http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0408\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0473\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0496\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0412\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0464\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0419\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0369\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0455\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0519\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0313\n"
     ]
    }
   ],
   "source": [
    "sim_options = sim_options = {'name': 'cosine',\n",
    "               'user_based': False  # compute  similarities between items\n",
    "               }\n",
    "collabKNN = surprise.KNNBasic(k=30, sim_options=sim_options)\n",
    "for trainset, testset in kSplit.split(data): #data leakage due to pre-processing before splitting?\n",
    "    collabKNN.fit(trainset)\n",
    "    predictionsKNN = collabKNN.test(testset)\n",
    "    rmseKNN.append(surprise.accuracy.rmse(predictionsKNN,verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  0.24300003051757812 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {21385: [('Virtus Bologna', 3.630952380952381),\n",
       "              ('VfL Bad Schwartau', 3.630952380952381),\n",
       "              ('Shawinigan Cataractes', 3.630952380952381),\n",
       "              (\"St Patrick's Athletic FC\", 3.630952380952381),\n",
       "              ('PEPO Lappeenranta', 3.630952380952381),\n",
       "              ('Kristianstads DFF (Women)', 3.630952380952381),\n",
       "              ('SC Freiburg Women', 3.630952380952381),\n",
       "              ('Gatineau Olympiques', 3.630952380952381),\n",
       "              ('Petaling Jaya City FC', 3.630952380952381),\n",
       "              ('Spartak Noginsk Women', 3.630952380952381)]})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_start = time()\n",
    "predictionsKNN = []\n",
    "\n",
    "for team in set(df['Team']):\n",
    "    predictionsKNN.append(collabKNN.predict(21385, team))\n",
    "\n",
    "top_n = get_top_n(predictionsKNN, n=10)\n",
    "print('Time elapsed: ', time() - time_start,'s')\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Vector Decomposition\n",
    "This algorithm was created by Simon Funk during the Netflix Prize, and it is called FunkSVD. The big idea behind this algorithm is you try to estimate the best latent factors for the ratings. So, if you have a 100k users and 10k books, you factor the 100k x 10k matrix into the number of factors. In turn, you would be making two 100k x 30 and 30 x 10k matrices. You multiply them together to get the predicted rating. This lets us optimize on the latent factors between users, such as users that are similiar together because they all rated action films, and latent factors between items, like book series like Goosebumps and Steven King. We multiply each of these to get the predicted rating.\n",
    "\n",
    "If you'd like to read more, look it up here: https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9808\n",
      "RMSE: 0.9872\n",
      "RMSE: 0.9866\n",
      "RMSE: 0.9876\n",
      "RMSE: 0.9877\n",
      "RMSE: 0.9785\n",
      "RMSE: 0.9869\n",
      "RMSE: 0.9866\n",
      "RMSE: 0.9856\n",
      "RMSE: 0.9912\n"
     ]
    }
   ],
   "source": [
    "funkSVD = surprise.prediction_algorithms.matrix_factorization.SVD(n_epochs=30, n_factors=100)\n",
    "\n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    funkSVD.fit(trainset) \n",
    "    predictionsSVD = funkSVD.test(testset)   \n",
    "    rmseSVD.append(surprise.accuracy.rmse(predictionsSVD, verbose=True)) #get root means squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  0.08399677276611328 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {21385: [('Kuban Holding', 3.5385588951585705),\n",
       "              ('Brisbane Roar', 3.504588551750979),\n",
       "              ('PFC Kuban', 3.446240657728788),\n",
       "              ('Indiana Pacers', 3.424433123406577),\n",
       "              ('Deportivo Pereira', 3.3924991126353987),\n",
       "              ('Boston Celtics', 3.392130081057103),\n",
       "              ('The Strongest', 3.377591093752082),\n",
       "              ('Fiorentina', 3.3578938732120793),\n",
       "              ('Detroit Pistons', 3.34038150527184),\n",
       "              ('AFK Csikszereda Miercurea Ciuc', 3.336929752513204)]})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_start = time()\n",
    "predictionsSVD = []\n",
    "\n",
    "for team in set(df['Team']):\n",
    "    predictionsSVD.append(funkSVD.predict(21385, team))\n",
    "\n",
    "top_n = get_top_n(predictionsSVD, n=10)\n",
    "print('Time elapsed: ', time() - time_start,'s')\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-clustering collaborative filtering.\n",
    "Co-clustering is where you cluster users and items together, using clustering techniques. You identify three clusters. You'll have to sum three things to get a predicted rating:\n",
    "1. You find the cluster for the specified rating of user u and item i, and identify the mean of that cluster. So you find the mean of cluster u_i.\n",
    "2. find the mean of the cluster of item i and subtract that from the average rating of that item.\n",
    "3. find the mean of cluster of user u and substract that from the average rating of that user. \n",
    "\n",
    "If you want to learn more about Co-Clustering, read more here: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.113.6458&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1653\n",
      "RMSE: 1.1509\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-22ac5c10e4b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcoClus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurprise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_algorithms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_clustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCoClustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cltr_u\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cltr_i\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkSplit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#iterate through the folds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcoClus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpredictionsCoClus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoClus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mrmseCo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msurprise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictionsCoClus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#get root means squared error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\co_clustering.pyx\u001b[0m in \u001b[0;36msurprise.prediction_algorithms.co_clustering.CoClustering.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\co_clustering.pyx\u001b[0m in \u001b[0;36msurprise.prediction_algorithms.co_clustering.CoClustering.compute_averages\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\surprise\\trainset.py\u001b[0m in \u001b[0;36mall_ratings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_ratings\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mur\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mu_ratings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_testset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coClus = surprise.prediction_algorithms.co_clustering.CoClustering(n_cltr_u=4, n_cltr_i=4, n_epochs=100) \n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    coClus.fit(trainset)\n",
    "    predictionsCoClus = coClus.test(testset)\n",
    "    rmseCo.append(surprise.accuracy.rmse(predictionsCoClus,verbose=True))#get root means squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "predictionscoClus = []\n",
    "\n",
    "for team in set(df['Team']):\n",
    "    predictionscoClus.append(coClus.predict(21385, team))\n",
    "\n",
    "top_n = get_top_n(predictionscoClus, n=10)\n",
    "print('Time elapsed: ', time() - time_start,'s')\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope One Collaborative Filtering Algorithm\n",
    "This algorithm computes the slope of each of the relevant items rated by a user, finds the difference, then computes the prediction. Its a blunt instrument, but its a good heuristic that might improve our ensemble method. You can read more here: https://arxiv.org/abs/cs/0702144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0680\n",
      "RMSE: 1.0687\n",
      "RMSE: 1.0730\n",
      "RMSE: 1.0693\n",
      "RMSE: 1.0732\n",
      "RMSE: 1.0794\n",
      "RMSE: 1.0741\n",
      "RMSE: 1.0787\n",
      "RMSE: 1.0700\n",
      "RMSE: 1.0770\n"
     ]
    }
   ],
   "source": [
    "slopeOne = surprise.prediction_algorithms.slope_one.SlopeOne()\n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    slopeOne.fit(trainset)\n",
    "    predictionsSlope = slopeOne.test(testset)\n",
    "    rmseSlope.append(surprise.accuracy.rmse(predictionsSlope,verbose=True))#get root means squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  0.2909975051879883 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {21385: [('CD Huarte', 5),\n",
       "              ('Beti Kozkor KE', 5),\n",
       "              ('Petaling Jaya City FC', 5),\n",
       "              ('WBC Wels', 5),\n",
       "              ('Oviedo Baloncesto', 5),\n",
       "              ('Klosterneuburg Dukes', 5),\n",
       "              ('Joker Swiecie Women', 5),\n",
       "              ('CA Talleres Remedios de Escalada', 5),\n",
       "              ('Hyogo Storks', 5),\n",
       "              ('Alians Lypova Dolyna', 4.913888888888888)]})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_start = time()\n",
    "predictionsslope = []\n",
    "\n",
    "for team in set(df['Team']):\n",
    "    predictionsslope.append(slopeOne.predict(21385, team))\n",
    "\n",
    "top_n = get_top_n(predictionsslope, n=10)\n",
    "print('Time elapsed: ', time() - time_start,'s')\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.973273</td>\n",
       "      <td>0.960936</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.891658</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.775183</td>\n",
       "      <td>0.891358</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.726702</td>\n",
       "      <td>0.886046</td>\n",
       "      <td>00:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.642304</td>\n",
       "      <td>0.875212</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.533611</td>\n",
       "      <td>0.871006</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.433984</td>\n",
       "      <td>0.867051</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.333522</td>\n",
       "      <td>0.867322</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.241643</td>\n",
       "      <td>0.869800</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.207458</td>\n",
       "      <td>0.870520</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = CollabDataLoaders.from_df(df, seed=42, valid_pct=0.2, user_name='IDUtente', item_name='Team', rating_name='Importo')\n",
    "learn = collab_learner(data, y_range=(0,5.5), n_factors=60)\n",
    "learn.fit_one_cycle(n_epoch=10, lr_max=5e-3, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDUtente</th>\n",
       "      <th>Team</th>\n",
       "      <th>Importo</th>\n",
       "      <th>Importo_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>860.0</td>\n",
       "      <td>3857.0</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>2.961366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3919.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>1.964286</td>\n",
       "      <td>3.164419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>868.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.567116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3865.0</td>\n",
       "      <td>2980.0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>2.158129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317.0</td>\n",
       "      <td>2494.0</td>\n",
       "      <td>2.672575</td>\n",
       "      <td>2.963706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1739.0</td>\n",
       "      <td>5492.0</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.847874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5683.0</td>\n",
       "      <td>4534.0</td>\n",
       "      <td>1.860987</td>\n",
       "      <td>4.486747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5453.0</td>\n",
       "      <td>5116.0</td>\n",
       "      <td>3.421053</td>\n",
       "      <td>3.186068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2518.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>3.092105</td>\n",
       "      <td>2.608530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our KNN is outperforming the rest. Lets try to hybridize the models so we can get the best parts of every model. To do this, we're going to use Suprise to make a new algorithm, and make it out-perform the rest.\n",
    "\n",
    "Now we'll make a class in Surprise and inherit it from Algobase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridFacto(surprise.AlgoBase):\n",
    "    def __init__(self, epochs, learning_rate):\n",
    "        self.alpha = np.array([0.25]*4)\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def fit(self, holdout):\n",
    "        holdout=holdout.build_full_trainset().build_testset()\n",
    "        \n",
    "        for epoch in range(self.epochs): \n",
    "            print('Starting epoch: ', epoch)\n",
    "                \n",
    "            predictions = np.array([collabKNN.test(holdout), funkSVD.test(holdout), coClus.test(holdout), slopeOne.test(holdout)])\n",
    "                        \n",
    "            maeGradient = [surprise.accuracy.mae([pred for pred in prediction]) for prediction in predictions] \n",
    "            \n",
    "            newalpha = self.alpha - np.transpose([self.learning_rate * mae for mae in maeGradient])\n",
    "            \n",
    "            #convergence check:\n",
    "            alpha_diff = [x-y for x,y in zip(newalpha, self.alpha)]\n",
    "            alpha_abs_mean = abs(np.mean(alpha_diff))\n",
    "             \n",
    "            print('alpha_abs_mean: ', alpha_abs_mean)\n",
    "            print('====================================')\n",
    "            \n",
    "            if alpha_abs_mean < 0.001:\n",
    "                break\n",
    "                    \n",
    "            self.alpha = newalpha\n",
    "            \n",
    "    def estimate(self,u,i):\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unknown.')\n",
    "        algoResults = np.array([collabKNN.predict(u,i),funkSVD.predict(u,i),coClus.predict(u,i),slopeOne.predict(u,i)])\n",
    "        return np.sum(np.dot(self.alpha,algoResults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Round predicted ratings\n",
    "\n",
    "\n",
    "class HybridFacto(surprise.AlgoBase):\n",
    "    def __init__(self, epochs, learning_rate):\n",
    "        self.alpha = np.array([0.25]*4)\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def fit(self, holdout):\n",
    "        holdout=holdout.build_full_trainset().build_testset()\n",
    "        \n",
    "        for epoch in range(self.epochs): \n",
    "            print('Starting epoch: ', epoch)\n",
    "                \n",
    "            predictions = np.array([collabKNN.test(holdout), funkSVD.test(holdout), coClus.test(holdout), slopeOne.test(holdout)])\n",
    "                 \n",
    "            print(predictions[0][0])\n",
    "                \n",
    "            maeGradient = [surprise.accuracy.rmse([pred for pred in prediction]) for prediction in predictions] \n",
    "            \n",
    "            \n",
    "            newalpha = self.alpha - np.transpose([self.learning_rate * mae for mae in maeGradient])\n",
    "            \n",
    "            #convergence check:\n",
    "            alpha_diff = [x-y for x,y in zip(newalpha, self.alpha)]\n",
    "            alpha_abs_mean = abs(np.mean(alpha_diff))\n",
    "             \n",
    "            print('alpha_abs_mean: ', alpha_abs_mean)\n",
    "            print('====================================')\n",
    "            \n",
    "            if alpha_abs_mean < 0.001:\n",
    "                break\n",
    "                    \n",
    "            self.alpha = newalpha\n",
    "            \n",
    "    def estimate(self,u,i):\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unknown.')\n",
    "        algoResults = np.array([collabKNN.predict(u,i), funkSVD.predict(u,i), coClus.predict(u,i), slopeOne.predict(u,i)])\n",
    "        return np.sum(np.dot(self.alpha,algoResults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "[11673 'CS Maritimo Funchal U23' 4.173640167364017 2.1523709902370993\n",
      " {'actual_k': 30, 'was_impossible': False}]\n",
      "RMSE: 1.4761\n",
      "RMSE: 1.4267\n",
      "RMSE: 1.6040\n",
      "RMSE: 1.5075\n",
      "alpha_abs_mean:  0.007517887943545264\n",
      "====================================\n",
      "Starting epoch:  1\n",
      "[11673 'CS Maritimo Funchal U23' 4.173640167364017 2.1523709902370993\n",
      " {'actual_k': 30, 'was_impossible': False}]\n",
      "RMSE: 1.4761\n",
      "RMSE: 1.4267\n",
      "RMSE: 1.6040\n",
      "RMSE: 1.5075\n",
      "alpha_abs_mean:  0.007517887943545264\n",
      "====================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HybridFacto' object has no attribute 'trainset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-9a97e1aed430>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrmseHyb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkSplit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#iterate through the folds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpredhybrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhybrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mrmseHyb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msurprise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredhybrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, testset, verbose)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# The ratings are translated back to their original scale.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         predictions = [self.predict(uid,\n\u001b[0m\u001b[0;32m    165\u001b[0m                                     \u001b[0miid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mr_ui_trans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# The ratings are translated back to their original scale.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         predictions = [self.predict(uid,\n\u001b[0m\u001b[0;32m    165\u001b[0m                                     \u001b[0miid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mr_ui_trans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, uid, iid, r_ui, clip, verbose)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m# Convert raw ids to inner ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0miuid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_inner_uid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0miuid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'UKN__'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HybridFacto' object has no attribute 'trainset'"
     ]
    }
   ],
   "source": [
    "holdout = surprise.Dataset.load_from_df(rawholdout, reader)\n",
    "hybrid = HybridFacto(epochs=2, learning_rate=0.005)\n",
    "hybrid.fit(holdout)\n",
    "\n",
    "rmseHyb = []\n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    predhybrid = hybrid.test(testset)\n",
    "    rmseHyb.append(surprise.accuracy.rmse(predhybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
